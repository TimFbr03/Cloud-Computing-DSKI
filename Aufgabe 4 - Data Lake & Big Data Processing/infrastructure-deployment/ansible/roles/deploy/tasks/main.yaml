# ==========================
# Install prerequisites (von Aufgabe 3)
# ==========================
- name: Install prerequisites
  apt:
    name:
      - curl
      - gnupg
      - lsb-release
      - python3-pip        # pip installieren
    update_cache: yes
    state: present

- name: Install Kubernetes Python module
  pip:
    name: kubernetes
    executable: pip3

# ==========================
# k3s server installation (von Aufgabe 3)
# ==========================
- name: Install k3s server
  shell: curl -sfL https://get.k3s.io | sh -s - --write-kubeconfig-mode 644
  args:
    creates: /usr/local/bin/k3s
  when: "'k3s_server' in group_names"

- name: Wait for k3s server to be ready
  shell: k3s kubectl get node
  register: k3s_status
  retries: 10
  delay: 30
  until: k3s_status.rc == 0
  when: "'k3s_server' in group_names"

- name: Save node token for agents
  command: cat /var/lib/rancher/k3s/server/node-token
  register: node_token
  when: "'k3s_server' in group_names"

- name: Copy token to localhost
  copy:
    content: "{{ node_token.stdout }}"
    dest: "./node-token"
    mode: '0600'
  delegate_to: localhost
  become: no
  when: "'k3s_server' in group_names"

# ==========================
# Helm installation (von Aufgabe 3)
# ==========================
- name: Install Helm (server only)
  shell: curl https://raw.githubusercontent.com/helm/helm/master/scripts/get-helm-3 | bash
  args:
    creates: /usr/local/bin/helm
  when: "'k3s_server' in group_names"

# ==========================
# Copy Helm charts to server (erweitert)
# ==========================
- name: Copy Helm charts to server
  copy:
    src: "{{ playbook_dir }}/../helm/"
    dest: "/tmp/helm"
    mode: '0644'
    remote_src: no
    directory_mode: '0755'
  when: "'k3s_server' in group_names"

# NEU: Create sample data files for ingestion
- name: Create sample data directory
  file:
    path: /tmp/sample-data
    state: directory
    mode: '0755'
  when: "'k3s_server' in group_names"

- name: Create sample Avro JSON data files
  copy:
    content: |
      {
        "schema": {
          "type": "record",
          "name": "Task",
          "fields": [
            {"name": "title", "type": "string"},
            {"name": "description", "type": "string"},
            {"name": "completed", "type": "boolean"},
            {"name": "deleted", "type": "boolean"},
            {"name": "user_id", "type": "string"}
          ]
        },
        "records": [
          {"title": "Task 1", "description": "Complete project documentation", "completed": true, "deleted": false, "user_id": "user1"},
          {"title": "Task 2", "description": "Review code changes", "completed": false, "deleted": false, "user_id": "user1"},
          {"title": "Task 3", "description": "Deploy to production", "completed": false, "deleted": false, "user_id": "user2"}
        ]
      }
    dest: /tmp/sample-data/avro_dataset1_project_tasks.json
    mode: '0644'
  when: "'k3s_server' in group_names"

- name: Create research activities sample data
  copy:
    content: |
      {
        "schema": {
          "type": "record",
          "name": "Task",
          "fields": [
            {"name": "title", "type": "string"},
            {"name": "description", "type": "string"},
            {"name": "completed", "type": "boolean"},
            {"name": "deleted", "type": "boolean"},
            {"name": "user_id", "type": "string"}
          ]
        },
        "records": [
          {"title": "AI Research", "description": "Machine Learning algorithm development", "completed": true, "deleted": false, "user_id": "researcher1"},
          {"title": "Quantum Computing", "description": "Quantum algorithm optimization", "completed": false, "deleted": false, "user_id": "researcher2"},
          {"title": "Blockchain Analysis", "description": "Smart contract security research", "completed": true, "deleted": false, "user_id": "researcher1"}
        ]
      }
    dest: /tmp/sample-data/avro_dataset2_research_activities.json
    mode: '0644'
  when: "'k3s_server' in group_names"

- name: Create schema file
  copy:
    content: |
      {
        "type": "record",
        "name": "Task",
        "fields": [
          {"name": "title", "type": "string"},
          {"name": "description", "type": "string"},
          {"name": "completed", "type": "boolean"},
          {"name": "deleted", "type": "boolean"},
          {"name": "user_id", "type": "string"}
        ]
      }
    dest: /tmp/sample-data/avro_schema_tasks.json
    mode: '0644'
  when: "'k3s_server' in group_names"

# NEU: Copy Data Ingestion Scripts (fixed)
- name: Copy data ingestion scripts
  copy:
    src: "{{ playbook_dir }}/../data-ingestion/"
    dest: "/tmp/data-ingestion"
    mode: '0755'
    remote_src: no
    directory_mode: '0755'
  when: "'k3s_server' in group_names"

# ==========================
# Deploy App Chart (von Aufgabe 3)
# ==========================
- name: Create namespace for app
  shell: /usr/local/bin/k3s kubectl create namespace microservices || true
  environment:
    KUBECONFIG: /etc/rancher/k3s/k3s.yaml
  when: "'k3s_server' in group_names"

- name: Deploy App Helm chart
  shell: |
    helm upgrade --install "{{ helm_release_name }}" "/tmp/helm/app" \
      --namespace microservices \
      --create-namespace \
      -f "/tmp/helm/app/values.yaml"
  environment:
    KUBECONFIG: /etc/rancher/k3s/k3s.yaml
  register: helm_app_result
  retries: 2
  delay: 10
  until: helm_app_result.rc == 0
  when: "'k3s_server' in group_names"

# ==========================
# Install Nginx Ingress Controller (von Aufgabe 3)
# ==========================
- name: Install Nginx Ingress Controller
  shell: |
    kubectl apply -f https://raw.githubusercontent.com/kubernetes/ingress-nginx/controller-v1.8.1/deploy/static/provider/cloud/deploy.yaml
  environment:
    KUBECONFIG: /etc/rancher/k3s/k3s.yaml
  when: "'k3s_server' in group_names"

# ==========================
# Deploy Monitoring Stack (von Aufgabe 3)
# ==========================
- name: Add Prometheus Helm repo
  shell: helm repo add prometheus-community https://prometheus-community.github.io/helm-charts && helm repo update
  environment:
    KUBECONFIG: /etc/rancher/k3s/k3s.yaml
  when: "'k3s_server' in group_names"

- name: Create monitoring namespace
  shell: /usr/local/bin/k3s kubectl create namespace monitoring || true
  environment:
    KUBECONFIG: /etc/rancher/k3s/k3s.yaml
  when: "'k3s_server' in group_names"

- name: Deploy kube-prometheus-stack
  shell: |
    helm upgrade --install monitoring prometheus-community/kube-prometheus-stack \
      --namespace monitoring \
      --create-namespace \
      --set grafana.service.type=NodePort \
      --set grafana.service.nodePort=30090 \
      --set grafana.adminPassword=admin123
  environment:
    KUBECONFIG: /etc/rancher/k3s/k3s.yaml
  register: helm_monitoring_result
  retries: 2
  delay: 10
  until: helm_monitoring_result.rc == 0
  when: "'k3s_server' in group_names"

# ==========================
# Wait for Monitoring Stack (von Aufgabe 3)
# ==========================
- name: Wait for Grafana pod to be ready
  shell: |
    kubectl -n monitoring wait --for=condition=Ready pod -l app.kubernetes.io/name=grafana,app.kubernetes.io/instance=monitoring --timeout=600s
  environment:
    KUBECONFIG: /etc/rancher/k3s/k3s.yaml
  when: "'k3s_server' in group_names"

- name: Wait for Prometheus pods to be ready
  shell: |
    kubectl -n monitoring wait --for=condition=Ready pod -l app.kubernetes.io/name=prometheus --timeout=600s
  environment:
    KUBECONFIG: /etc/rancher/k3s/k3s.yaml
  when: "'k3s_server' in group_names"

# ==========================
# k3s agent installation (von Aufgabe 3)
# ==========================
- name: Install k3s agent
  shell: |
    curl -sfL https://get.k3s.io | K3S_URL="https://{{ hostvars[groups['k3s_server'][0]].inventory_hostname }}:6443" \
    K3S_TOKEN="{{ lookup('file', './node-token') }}" sh -
  args:
    creates: /usr/local/bin/k3s
  when: "'k3s_agent' in group_names"

# ==========================
# NEU: Hadoop Data Lake Deployment (FIXED)
# ==========================
- name: Ensure Hadoop data directories exist on all nodes
  file:
    path: "{{ item }}"
    state: directory
    owner: root
    group: root
    mode: '0755'
  loop:
    - /data/hadoop/namenode
    - /data/hadoop/datanode-1
    - /data/hadoop/datanode-2
    - /data/hadoop/datanode-3

# Create namespace manually first
- name: Create Hadoop namespace
  shell: |
    kubectl create namespace {{ hadoop.namespace }} || true
  environment:
    KUBECONFIG: /etc/rancher/k3s/k3s.yaml
  when: "'k3s_server' in group_names"

# Deploy storage resources first
- name: Deploy Hadoop storage resources
  shell: |
    kubectl apply -f /tmp/helm/data_lake/templates/storage.yaml
  environment:
    KUBECONFIG: /etc/rancher/k3s/k3s.yaml
  when: "'k3s_server' in group_names"

# Deploy config map
- name: Deploy Hadoop configuration
  shell: |
    kubectl apply -f /tmp/helm/data_lake/templates/hadoop-config.yaml
  environment:
    KUBECONFIG: /etc/rancher/k3s/k3s.yaml
  when: "'k3s_server' in group_names"

# Deploy NameNode first
- name: Deploy Hadoop NameNode
  shell: |
    kubectl apply -f /tmp/helm/data_lake/templates/namenode.yaml
  environment:
    KUBECONFIG: /etc/rancher/k3s/k3s.yaml
  when: "'k3s_server' in group_names"

- name: Wait for NameNode pod to exist
  shell: kubectl get pod -l app=hadoop-namenode -n hadoop-cluster
  register: namenode_pod
  retries: 10
  delay: 5
  until: namenode_pod.rc == 0

- name: Wait for Hadoop NameNode StatefulSet to be ready
  shell: kubectl rollout status statefulset/hadoop-namenode -n {{ hadoop.namespace }} --timeout=600s
  environment:
    KUBECONFIG: /etc/rancher/k3s/k3s.yaml
  when: "'k3s_server' in group_names"
  
# Deploy DataNodes
- name: Deploy Hadoop DataNodes
  shell: |
    kubectl apply -f /tmp/helm/data_lake/templates/datanode.yaml
  environment:
    KUBECONFIG: /etc/rancher/k3s/k3s.yaml
  when: "'k3s_server' in group_names"

- name: Wait for DataNodes to be ready
  shell: |
    kubectl wait --for=condition=ready pod -l app=hadoop-datanode -n {{ hadoop.namespace }} --timeout=600s
  environment:
    KUBECONFIG: /etc/rancher/k3s/k3s.yaml
  when: "'k3s_server' in group_names"

# ==========================
# NEU: HDFS Initialization (FIXED)
# ==========================
- name: Check if HDFS is already formatted
  shell: |
    kubectl exec -n {{ hadoop.namespace }} deployment/hadoop-namenode -- test -d /hadoop/dfs/name/current
  environment:
    KUBECONFIG: /etc/rancher/k3s/k3s.yaml
  register: hdfs_formatted_check
  failed_when: false
  when: "'k3s_server' in group_names"

- name: Format HDFS NameNode (first time only)
  shell: |
    kubectl exec -n {{ hadoop.namespace }} deployment/hadoop-namenode -- /opt/hadoop/bin/hdfs namenode -format -force -Y
  environment:
    KUBECONFIG: /etc/rancher/k3s/k3s.yaml
  when: 
    - "'k3s_server' in group_names"
    - hdfs_formatted_check.rc != 0

- name: Wait a bit for HDFS to be fully ready
  pause:
    seconds: 30
  when: "'k3s_server' in group_names"

- name: Create HDFS directory structure
  shell: |
    kubectl exec -n {{ hadoop.namespace }} deployment/hadoop-namenode -- /opt/hadoop/bin/hdfs dfs -mkdir -p {{ item }}
  environment:
    KUBECONFIG: /etc/rancher/k3s/k3s.yaml
  loop: "{{ data_ingestion.hdfs_directories }}"
  ignore_errors: yes
  when: "'k3s_server' in group_names"

# ==========================
# NEU: Data Ingestion (FIXED - using kubectl apply)
# ==========================
- name: Create simple data ingestion ConfigMap
  shell: |
    kubectl create configmap sample-data \
      --from-file=/tmp/sample-data/avro_dataset1_project_tasks.json \
      --from-file=/tmp/sample-data/avro_dataset2_research_activities.json \
      --from-file=/tmp/sample-data/avro_schema_tasks.json \
      -n {{ hadoop.namespace }} --dry-run=client -o yaml | kubectl apply -f -
  environment:
    KUBECONFIG: /etc/rancher/k3s/k3s.yaml
  when: "'k3s_server' in group_names"

- name: Create data ingestion job manifest
  copy:
    content: |
      apiVersion: v1
      kind: Pod
      metadata:
        name: data-ingestion
        namespace: {{ hadoop.namespace }}
      spec:
        restartPolicy: Never
        volumes:
          - name: sample-data
            configMap:
              name: sample-data
          - name: hadoop-config
            configMap:
              name: hadoop-config
        containers:
          - name: data-ingestion
            image: apache/hadoop:3
            command: ["/bin/bash", "-c"]
            args:
              - |
                set -e
                echo "Starting data ingestion..."
                cp /data/* /tmp/
                ls -la /tmp/
                
                echo "Putting project tasks data..."
                /opt/hadoop/bin/hdfs dfs -put /tmp/avro_dataset1_project_tasks.json /datalake/datasets/project_tasks/ || echo "Project tasks already exists"
                
                echo "Putting research activities data..."
                /opt/hadoop/bin/hdfs dfs -put /tmp/avro_dataset2_research_activities.json /datalake/datasets/research_activities/ || echo "Research activities already exists"
                
                echo "Putting schema data..."
                /opt/hadoop/bin/hdfs dfs -put /tmp/avro_schema_tasks.json /datalake/schemas/ || echo "Schema already exists"
                
                echo "Data ingestion completed successfully!"
                sleep 10
            volumeMounts:
              - name: sample-data
                mountPath: /data
              - name: hadoop-config
                mountPath: /opt/hadoop/etc/hadoop
    dest: /tmp/data-ingestion-pod.yaml
    mode: '0644'
  when: "'k3s_server' in group_names"

- name: Run data ingestion job
  shell: |
    kubectl delete pod data-ingestion -n {{ hadoop.namespace }} --ignore-not-found=true
    kubectl apply -f /tmp/data-ingestion-pod.yaml
    kubectl wait --for=condition=ready pod/data-ingestion -n {{ hadoop.namespace }} --timeout=300s || true
    kubectl logs data-ingestion -n {{ hadoop.namespace }}
    kubectl delete pod data-ingestion -n {{ hadoop.namespace }} --ignore-not-found=true
  environment:
    KUBECONFIG: /etc/rancher/k3s/k3s.yaml
  when: "'k3s_server' in group_names"

- name: Verify data ingestion
  shell: |
    kubectl exec -n {{ hadoop.namespace }} deployment/hadoop-namenode -- /opt/hadoop/bin/hdfs dfs -ls -R /datalake/datasets/ || echo "Could not list datasets"
  environment:
    KUBECONFIG: /etc/rancher/k3s/k3s.yaml
  when: "'k3s_server' in group_names"

# ==========================
# Label nodes for better scheduling
# ==========================
- name: Get node names
  command: kubectl get nodes -o jsonpath='{.items[*].metadata.name}'
  register: k8s_nodes
  delegate_to: "{{ groups['k3s_server'][0] }}"
  run_once: true
  environment:
    KUBECONFIG: /etc/rancher/k3s/k3s.yaml

- name: Label nodes for Hadoop workloads
  command: kubectl label node {{ item }} role=hadoop-worker --overwrite
  loop: "{{ k8s_nodes.stdout.split() }}"
  delegate_to: "{{ groups['k3s_server'][0] }}"
  environment:
    KUBECONFIG: /etc/rancher/k3s/k3s.yaml

# ==========================
# NEU: Deployment Summary
# ==========================
# Ergänzung zu main.yaml - Task für das Deployment Summary Update
- name: Display deployment summary
  debug:
    msg: |
      ============================================
      Hadoop Data Lake Deployment Complete!
      ============================================
      
      Microservices (from Task 3):
      - Frontend: Available via Ingress
      - Backend: Available via Ingress
      - Database: Internal service
      
      Monitoring:
      - Grafana: http://{{ ansible_default_ipv4.address }}:30090
      - Prometheus: Available via port-forward
      
      Hadoop Data Lake - External Access:
      - NameNode Web UI: http://{{ ansible_default_ipv4.address }}:30091
      - YARN ResourceManager: http://{{ ansible_default_ipv4.address }}:30092
      - Spark History Server: http://{{ ansible_default_ipv4.address }}:30093
      
      Hadoop Data Lake - Internal Services:
      - NameNode RPC: hadoop-namenode.hadoop-cluster.svc.cluster.local:9000
      - ResourceManager: hadoop-resourcemanager.hadoop-cluster.svc.cluster.local:8088
      
      Data Processing:
      - Spark Jobs: Check with kubectl get jobs -n {{ hadoop.namespace }}
      - Data Location: HDFS at /datalake/
      
      Access Commands:
      1. Check HDFS status: kubectl exec -n {{ hadoop.namespace }} deployment/hadoop-namenode -- /opt/hadoop/bin/hdfs dfsadmin -report
      2. View data: kubectl exec -n {{ hadoop.namespace }} deployment/hadoop-namenode -- /opt/hadoop/bin/hdfs dfs -ls /datalake
      3. Check Spark jobs: kubectl logs -n {{ hadoop.namespace }} job/spark-task-processing
      4. Port-forward (alternative access): kubectl port-forward -n {{ hadoop.namespace }} svc/hadoop-namenode-web 9870:9870
      
      Web UIs Available:
      - NameNode: Browse HDFS, view cluster status, file system operations
      - ResourceManager: Monitor YARN applications, resource allocation
      - Spark History: View completed Spark job details and metrics
  when: "'k3s_server' in group_names"

# Zusätzlicher Task um sicherzustellen, dass alle NodePort Services bereit sind
- name: Wait for Hadoop Web UIs to be accessible
  uri:
    url: "http://{{ ansible_default_ipv4.address }}:{{ item }}"
    method: GET
    status_code: [200, 302]
  register: ui_check
  until: ui_check.status in [200, 302]
  retries: 10
  delay: 30
  loop:
    - 30091  # NameNode Web UI
    - 30092  # ResourceManager Web UI
    - 30093  # Spark History Server
  when: "'k3s_server' in group_names"
  ignore_errors: yes
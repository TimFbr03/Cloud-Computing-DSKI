# ==========================
# Install prerequisites (von Aufgabe 3)
# ==========================
- name: Install prerequisites
  apt:
    name:
      - curl
      - gnupg
      - lsb-release
      - python3-pip        # pip installieren
    update_cache: yes
    state: present

- name: Install Kubernetes Python module
  pip:
    name: kubernetes
    executable: pip3

# ==========================
# k3s server installation (von Aufgabe 3)
# ==========================
- name: Install k3s server
  shell: curl -sfL https://get.k3s.io | sh -s - --write-kubeconfig-mode 644
  args:
    creates: /usr/local/bin/k3s
  when: "'k3s_server' in group_names"

- name: Wait for k3s server to be ready
  shell: k3s kubectl get node
  register: k3s_status
  retries: 10
  delay: 30
  until: k3s_status.rc == 0
  when: "'k3s_server' in group_names"

- name: Save node token for agents
  command: cat /var/lib/rancher/k3s/server/node-token
  register: node_token
  when: "'k3s_server' in group_names"

- name: Copy token to localhost
  copy:
    content: "{{ node_token.stdout }}"
    dest: "./node-token"
    mode: '0600'
  delegate_to: localhost
  become: no
  when: "'k3s_server' in group_names"

# ==========================
# Helm installation (von Aufgabe 3)
# ==========================
- name: Install Helm (server only)
  shell: curl https://raw.githubusercontent.com/helm/helm/master/scripts/get-helm-3 | bash
  args:
    creates: /usr/local/bin/helm
  when: "'k3s_server' in group_names"

# ==========================
# Copy Helm charts to server (erweitert)
# ==========================
- name: Copy Helm charts to server
  copy:
    src: "{{ playbook_dir }}/../helm/"
    dest: "/tmp/helm"
    mode: '0644'
    remote_src: no
    directory_mode: '0755'
  when: "'k3s_server' in group_names"

# NEU: Create sample data files for ingestion
- name: Create sample data directory
  file:
    path: /tmp/sample-data
    state: directory
    mode: '0755'
  when: "'k3s_server' in group_names"

- name: Create sample Avro JSON data files
  copy:
    content: |
      {
        "schema": {
          "type": "record",
          "name": "Task",
          "fields": [
            {"name": "title", "type": "string"},
            {"name": "description", "type": "string"},
            {"name": "completed", "type": "boolean"},
            {"name": "deleted", "type": "boolean"},
            {"name": "user_id", "type": "string"}
          ]
        },
        "records": [
          {"title": "Task 1", "description": "Complete project documentation", "completed": true, "deleted": false, "user_id": "user1"},
          {"title": "Task 2", "description": "Review code changes", "completed": false, "deleted": false, "user_id": "user1"},
          {"title": "Task 3", "description": "Deploy to production", "completed": false, "deleted": false, "user_id": "user2"}
        ]
      }
    dest: /tmp/sample-data/avro_dataset1_project_tasks.json
    mode: '0644'
  when: "'k3s_server' in group_names"

- name: Create research activities sample data
  copy:
    content: |
      {
        "schema": {
          "type": "record",
          "name": "Task",
          "fields": [
            {"name": "title", "type": "string"},
            {"name": "description", "type": "string"},
            {"name": "completed", "type": "boolean"},
            {"name": "deleted", "type": "boolean"},
            {"name": "user_id", "type": "string"}
          ]
        },
        "records": [
          {"title": "AI Research", "description": "Machine Learning algorithm development", "completed": true, "deleted": false, "user_id": "researcher1"},
          {"title": "Quantum Computing", "description": "Quantum algorithm optimization", "completed": false, "deleted": false, "user_id": "researcher2"},
          {"title": "Blockchain Analysis", "description": "Smart contract security research", "completed": true, "deleted": false, "user_id": "researcher1"}
        ]
      }
    dest: /tmp/sample-data/avro_dataset2_research_activities.json
    mode: '0644'
  when: "'k3s_server' in group_names"

- name: Create schema file
  copy:
    content: |
      {
        "type": "record",
        "name": "Task",
        "fields": [
          {"name": "title", "type": "string"},
          {"name": "description", "type": "string"},
          {"name": "completed", "type": "boolean"},
          {"name": "deleted", "type": "boolean"},
          {"name": "user_id", "type": "string"}
        ]
      }
    dest: /tmp/sample-data/avro_schema_tasks.json
    mode: '0644'
  when: "'k3s_server' in group_names"

# NEU: Copy Data Ingestion Scripts (fixed)
- name: Copy data ingestion scripts
  copy:
    src: "{{ playbook_dir }}/../data-ingestion/"
    dest: "/tmp/data-ingestion"
    mode: '0755'
    remote_src: no
    directory_mode: '0755'
  when: "'k3s_server' in group_names"

# ==========================
# Deploy App Chart (von Aufgabe 3)
# ==========================
- name: Create namespace for app
  shell: /usr/local/bin/k3s kubectl create namespace microservices || true
  environment:
    KUBECONFIG: /etc/rancher/k3s/k3s.yaml
  when: "'k3s_server' in group_names"

- name: Deploy App Helm chart
  shell: |
    helm upgrade --install "{{ helm_release_name }}" "/tmp/helm/app" \
      --namespace microservices \
      --create-namespace \
      -f "/tmp/helm/app/values.yaml"
  environment:
    KUBECONFIG: /etc/rancher/k3s/k3s.yaml
  register: helm_app_result
  retries: 2
  delay: 10
  until: helm_app_result.rc == 0
  when: "'k3s_server' in group_names"

# ==========================
# Install Nginx Ingress Controller (von Aufgabe 3)
# ==========================
- name: Install Nginx Ingress Controller
  shell: |
    kubectl apply -f https://raw.githubusercontent.com/kubernetes/ingress-nginx/controller-v1.8.1/deploy/static/provider/cloud/deploy.yaml
  environment:
    KUBECONFIG: /etc/rancher/k3s/k3s.yaml
  when: "'k3s_server' in group_names"

# ==========================
# Deploy Monitoring Stack (von Aufgabe 3)
# ==========================
- name: Add Prometheus Helm repo
  shell: helm repo add prometheus-community https://prometheus-community.github.io/helm-charts && helm repo update
  environment:
    KUBECONFIG: /etc/rancher/k3s/k3s.yaml
  when: "'k3s_server' in group_names"

- name: Create monitoring namespace
  shell: /usr/local/bin/k3s kubectl create namespace monitoring || true
  environment:
    KUBECONFIG: /etc/rancher/k3s/k3s.yaml
  when: "'k3s_server' in group_names"

- name: Deploy kube-prometheus-stack
  shell: |
    helm upgrade --install monitoring prometheus-community/kube-prometheus-stack \
      --namespace monitoring \
      --create-namespace \
      --set grafana.service.type=NodePort \
      --set grafana.service.nodePort=30090 \
      --set grafana.adminPassword=admin123
  environment:
    KUBECONFIG: /etc/rancher/k3s/k3s.yaml
  register: helm_monitoring_result
  retries: 2
  delay: 10
  until: helm_monitoring_result.rc == 0
  when: "'k3s_server' in group_names"

# ==========================
# Wait for Monitoring Stack (von Aufgabe 3)
# ==========================
- name: Wait for Grafana pod to be ready
  shell: |
    kubectl -n monitoring wait --for=condition=Ready pod -l app.kubernetes.io/name=grafana,app.kubernetes.io/instance=monitoring --timeout=600s
  environment:
    KUBECONFIG: /etc/rancher/k3s/k3s.yaml
  when: "'k3s_server' in group_names"

- name: Wait for Prometheus pods to be ready
  shell: |
    kubectl -n monitoring wait --for=condition=Ready pod -l app.kubernetes.io/name=prometheus --timeout=600s
  environment:
    KUBECONFIG: /etc/rancher/k3s/k3s.yaml
  when: "'k3s_server' in group_names"

# ==========================
# k3s agent installation (von Aufgabe 3)
# ==========================
- name: Install k3s agent
  shell: |
    curl -sfL https://get.k3s.io | K3S_URL="https://{{ hostvars[groups['k3s_server'][0]].inventory_hostname }}:6443" \
    K3S_TOKEN="{{ lookup('file', './node-token') }}" sh -
  args:
    creates: /usr/local/bin/k3s
  when: "'k3s_agent' in group_names"

# ==========================
# NEU: Hadoop Data Lake Deployment (FIXED fÃ¼r lokalen Speicher)
# ==========================

# Entferne die lokalen Verzeichnisse, da wir emptyDir verwenden
# - name: Ensure Hadoop data directories exist on all nodes
#   file:
#     path: "{{ item }}"
#     state: directory
#     owner: root
#     group: root
#     mode: '0755'
#   loop:
#     - /data/hadoop/namenode
#     - /data/hadoop/datanode-1
#     - /data/hadoop/datanode-2
#     - /data/hadoop/datanode-3

# Create namespace manually first
- name: Create Hadoop namespace
  shell: |
    kubectl create namespace {{ hadoop.namespace }} || true
  environment:
    KUBECONFIG: /etc/rancher/k3s/k3s.yaml
  when: "'k3s_server' in group_names"

# Deploy storage resources first (simplified)
- name: Deploy Hadoop storage resources
  shell: |
    kubectl apply -f /tmp/helm/data_lake/templates/storage.yaml
  environment:
    KUBECONFIG: /etc/rancher/k3s/k3s.yaml
  when: "'k3s_server' in group_names"

# Deploy config map
- name: Deploy Hadoop configuration
  shell: |
    kubectl apply -f /tmp/helm/data_lake/templates/hadoop-config.yaml
  environment:
    KUBECONFIG: /etc/rancher/k3s/k3s.yaml
  when: "'k3s_server' in group_names"

# Deploy NameNode first
- name: Deploy Hadoop NameNode
  shell: |
    kubectl apply -f /tmp/helm/data_lake/templates/namenode.yaml
  environment:
    KUBECONFIG: /etc/rancher/k3s/k3s.yaml
  when: "'k3s_server' in group_names"

- name: Wait for NameNode pod to be created
  shell: kubectl get pod -l app=hadoop-namenode -n {{ hadoop.namespace }} --no-headers | wc -l
  register: namenode_pod_count
  retries: 20
  delay: 10
  until: namenode_pod_count.stdout|int > 0
  environment:
    KUBECONFIG: /etc/rancher/k3s/k3s.yaml
  when: "'k3s_server' in group_names"

- name: Wait for Hadoop NameNode to be ready
  shell: kubectl wait --for=condition=ready pod/hadoop-namenode-0 -n {{ hadoop.namespace }} --timeout=300s
  environment:
    KUBECONFIG: /etc/rancher/k3s/k3s.yaml
  register: namenode_ready
  retries: 3
  delay: 30
  until: namenode_ready.rc == 0
  when: "'k3s_server' in group_names"

# Deploy DataNodes
- name: Deploy Hadoop DataNodes
  shell: |
    kubectl apply -f /tmp/helm/data_lake/templates/datanode.yaml
  environment:
    KUBECONFIG: /etc/rancher/k3s/k3s.yaml
  when: "'k3s_server' in group_names"

- name: Wait for DataNodes to be ready
  shell: |
    kubectl wait --for=condition=ready pod -l app=hadoop-datanode -n {{ hadoop.namespace }} --timeout=600s
  environment:
    KUBECONFIG: /etc/rancher/k3s/k3s.yaml
  register: datanode_ready
  retries: 3
  delay: 45
  until: datanode_ready.rc == 0
  when: "'k3s_server' in group_names"

# ==========================
# NEU: HDFS Initialization (FIXED fÃ¼r emptyDir)
# ==========================
- name: Wait for HDFS to be fully operational
  shell: |
    kubectl exec -n {{ hadoop.namespace }} hadoop-namenode-0 -- /opt/hadoop/bin/hdfs dfsadmin -report
  environment:
    KUBECONFIG: /etc/rancher/k3s/k3s.yaml
  register: hdfs_report
  retries: 10
  delay: 15
  until: hdfs_report.rc == 0
  when: "'k3s_server' in group_names"

# HDFS-Verzeichnisse anlegen (vereinfacht)
- name: Create HDFS directory structure
  shell: |
    kubectl exec -n {{ hadoop.namespace }} hadoop-namenode-0 -- /opt/hadoop/bin/hdfs dfs -mkdir -p {{ item }}
  loop:
    - /datalake
    - /datalake/datasets
    - /datalake/raw
    - /datalake/processed
  register: hdfs_dirs
  retries: 3
  delay: 10
  until: hdfs_dirs.rc == 0
  ignore_errors: yes
  when: "'k3s_server' in group_names"

# ==========================
# NEU: Simplified Data Ingestion (FIXED)
# ==========================
- name: Create simple data ingestion job manifest
  copy:
    content: |
      apiVersion: batch/v1
      kind: Job
      metadata:
        name: data-ingestion
        namespace: {{ hadoop.namespace }}
      spec:
        template:
          spec:
            restartPolicy: Never
            containers:
              - name: data-ingestion
                image: apache/hadoop:3
                command: ["/bin/bash", "-c"]
                args:
                  - |
                    set -e
                    echo "Starting simplified data ingestion..."
                    
                    # Create test data directly in container
                    cat > /tmp/test_data.json << 'EOF'
                    {"title": "Sample Task", "description": "Test data for Hadoop", "completed": false, "user_id": "test_user"}
                    EOF
                    
                    echo "Created test data file"
                    ls -la /tmp/test_data.json
                    
                    # Wait for HDFS to be ready
                    until /opt/hadoop/bin/hdfs dfsadmin -report > /dev/null 2>&1; do
                      echo "Waiting for HDFS..."
                      sleep 10
                    done
                    
                    echo "HDFS is ready, uploading data..."
                    /opt/hadoop/bin/hdfs dfs -mkdir -p /datalake/datasets/ || true
                    /opt/hadoop/bin/hdfs dfs -put /tmp/test_data.json /datalake/datasets/ || echo "File might already exist"
                    
                    echo "Verifying data upload..."
                    /opt/hadoop/bin/hdfs dfs -ls /datalake/datasets/
                    
                    echo "Data ingestion completed successfully!"
                    sleep 5
                volumeMounts:
                  - name: hadoop-config
                    mountPath: /opt/hadoop/etc/hadoop
                resources:
                  requests:
                    memory: "512Mi"
                    cpu: "250m"
                  limits:
                    memory: "1Gi"
                    cpu: "500m"
            volumes:
              - name: hadoop-config
                configMap:
                  name: hadoop-config
        backoffLimit: 2
    dest: /tmp/data-ingestion-job.yaml
    mode: '0644'
  when: "'k3s_server' in group_names"

- name: Run simplified data ingestion job
  shell: |
    kubectl delete job data-ingestion -n {{ hadoop.namespace }} --ignore-not-found=true
    sleep 5
    kubectl apply -f /tmp/data-ingestion-job.yaml
    kubectl wait --for=condition=complete job/data-ingestion -n {{ hadoop.namespace }} --timeout=300s || true
  environment:
    KUBECONFIG: /etc/rancher/k3s/k3s.yaml
  when: "'k3s_server' in group_names"

- name: Show data ingestion logs
  shell: |
    kubectl logs job/data-ingestion -n {{ hadoop.namespace }}
  environment:
    KUBECONFIG: /etc/rancher/k3s/k3s.yaml
  when: "'k3s_server' in group_names"

- name: Verify data ingestion
  shell: |
    kubectl exec -n {{ hadoop.namespace }} hadoop-namenode-0 -- /opt/hadoop/bin/hdfs dfs -ls -R /datalake/ || echo "Could not list datasets"
  environment:
    KUBECONFIG: /etc/rancher/k3s/k3s.yaml
  when: "'k3s_server' in group_names"

# ==========================
# Updated Deployment Summary (FIXED)
# ==========================
- name: Display deployment summary
  debug:
    msg: |
      ============================================
      Hadoop Data Lake Deployment Complete!
      ============================================
      
      â ï¸  IMPORTANT: Using ephemeral storage (emptyDir)
      - Data will be lost when pods restart
      - Suitable for development/testing only
      - Total available storage: ~6GB
      
      Microservices:
      - Frontend: Available via Ingress
      - Backend: Available via Ingress
      - Database: Internal service
      
      Monitoring:
      - Grafana: http://{{ ansible_default_ipv4.address }}:30090
      
      Hadoop Data Lake - External Access:
      - NameNode Web UI: http://{{ ansible_default_ipv4.address }}:30091
      
      Access Commands:
      1. Check HDFS status: kubectl exec -n {{ hadoop.namespace }} hadoop-namenode-0 -- /opt/hadoop/bin/hdfs dfsadmin -report
      2. View data: kubectl exec -n {{ hadoop.namespace }} hadoop-namenode-0 -- /opt/hadoop/bin/hdfs dfs -ls -R /datalake
      3. Check storage usage: kubectl exec -n {{ hadoop.namespace }} hadoop-namenode-0 -- df -h /hadoop/dfs/name
      
      Storage Monitoring:
      - NameNode storage: kubectl exec -n {{ hadoop.namespace }} hadoop-namenode-0 -- df -h /hadoop/dfs/name
      - DataNode storage: kubectl exec -n {{ hadoop.namespace }} hadoop-datanode-0 -- df -h /hadoop/dfs/data
      
      â ï¸  Remember: This is ephemeral storage - data will not survive pod restarts!
  when: "'k3s_server' in group_names"
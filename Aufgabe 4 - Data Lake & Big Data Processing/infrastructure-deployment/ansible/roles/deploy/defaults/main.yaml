# Docker Konfiguration
docker_packages:
  - docker.io

# Container Images
ghcr_images:
  backend: "ghcr.io/timfbr03/cloud-computing-dski/backend:latest"
  frontend: "ghcr.io/timfbr03/cloud-computing-dski/frontend:latest"
  database: "ghcr.io/timfbr03/cloud-computing-dski/database:latest"

# Helm Konfiguration f端r Microservices
helm_chart_path: "/opt/helm/microservices"
helm_release_name: "microservices"
helm_namespace: "microservices"

# Prometheus & Monitoring Konfiguration
monitoring_namespace: "monitoring"
prometheus_release_name: "prometheus"

# Grafana Konfiguration
grafana_admin_password: "admin123"  # In Produktion: verwende Ansible Vault!

# Prometheus Storage Konfiguration
prometheus_retention: "30d"
prometheus_storage_size: "10Gi"
prometheus_storage_class: "local-path"

# Grafana Storage Konfiguration
grafana_storage_size: "5Gi"
grafana_storage_class: "local-path"

# AlertManager Storage Konfiguration
alertmanager_storage_size: "2Gi"
alertmanager_storage_class: "local-path"

# Helm Repository URLs
helm_repositories:
  prometheus: "https://prometheus-community.github.io/helm-charts"
  grafana: "https://grafana.github.io/helm-charts"

# Ingress Konfiguration
ingress_class: "nginx"
monitoring_ingress_paths:
  prometheus: "/prometheus"
  grafana: "/grafana"
  alertmanager: "/alertmanager"

# k3s Konfiguration
k3s_write_kubeconfig_mode: "644"
k3s_token_file: "/tmp/node-token"
k3s_server_port: "6443"

# Timeouts und Retries
helm_install_timeout: "600s"
k3s_ready_retries: 3
k3s_ready_delay: 30
helm_install_retries: 3
helm_install_delay: 60

# Nginx Ingress Controller Version
nginx_ingress_version: "controller-v1.8.1"

# Hadoop Cluster Konfiguration
hadoop:
  namespace: "hadoop-cluster"
  chart_path: "/tmp/helm/data_lake"
  release_name: "hadoop-data-lake"
  
  # HDFS Konfiguration
  hdfs:
    replication_factor: 2
    block_size: "128m"
    namenode_heap_size: "2g"
    datanode_heap_size: "2g"
    
  # YARN Konfiguration
  yarn:
    resource_manager_heap_size: "2g"
    node_manager_heap_size: "2g"
    
  # Storage Konfiguration
  storage:
    namenode_size: "20Gi"
    datanode_size: "100Gi"
    storage_class: "local-path"
    
  # Services
  services:
    namenode_ui_port: 30870
    resource_manager_ui_port: 30888

# Spark Konfiguration f端r Big Data Processing
spark:
  namespace: "{{ hadoop.namespace }}"
  executor_instances: 2
  executor_cores: 2
  executor_memory: "2g"
  driver_memory: "1g"
  
# Data Ingestion Konfiguration
data_ingestion:
  datasets:
    - name: "project_tasks"
      source: "/tmp/data/avro_dataset1_project_tasks.json"
      hdfs_path: "/datalake/datasets/project_tasks/"
    - name: "research_activities"
      source: "/tmp/data/avro_dataset2_research_activities.json"
      hdfs_path: "/datalake/datasets/research_activities/"
    - name: "schema"
      source: "/tmp/data/avro_schema_tasks.json"
      hdfs_path: "/datalake/schemas/"
  
  # HDFS Verzeichnisstruktur
  hdfs_directories:
    - "/datalake"
    - "/datalake/datasets"
    - "/datalake/datasets/project_tasks"
    - "/datalake/datasets/research_activities"
    - "/datalake/schemas"
    - "/datalake/raw"
    - "/datalake/processed"
    - "/datalake/archive"
    - "/datalake/metadata"

# Container Images f端r Big Data
bigdata_images:
  hadoop: "apache/hadoop:3"
  spark: "bitnami/spark:3.4"
  zeppelin: "apache/zeppelin:0.10.1"

# Monitoring f端r Hadoop (Erweiterung der bestehenden Prometheus Config)
hadoop_monitoring:
  enabled: true
  scrape_interval: "30s"
  metrics_endpoints:
    - "hadoop-namenode:9870"
    - "hadoop-resourcemanager:8088"
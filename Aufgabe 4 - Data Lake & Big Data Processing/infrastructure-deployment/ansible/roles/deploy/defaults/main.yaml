# Docker Konfiguration
docker_packages:
  - docker.io

# Container Images
ghcr_images:
  backend: "ghcr.io/timfbr03/cloud-computing-dski/backend:latest"
  frontend: "ghcr.io/timfbr03/cloud-computing-dski/frontend:latest"
  database: "ghcr.io/timfbr03/cloud-computing-dski/database:latest"

# Helm Konfiguration für Microservices
helm_chart_path: "/opt/helm/microservices"
helm_release_name: "microservices"
helm_namespace: "microservices"

# Prometheus & Monitoring Konfiguration
monitoring_namespace: "monitoring"
prometheus_release_name: "prometheus"

# Grafana Konfiguration
grafana_admin_password: "admin123"  # In Produktion: verwende Ansible Vault!

# Prometheus Storage Konfiguration
prometheus_retention: "30d"
prometheus_storage_size: "10Gi"
prometheus_storage_class: "local-path"

# Grafana Storage Konfiguration
grafana_storage_size: "5Gi"
grafana_storage_class: "local-path"

# AlertManager Storage Konfiguration
alertmanager_storage_size: "2Gi"
alertmanager_storage_class: "local-path"

# Helm Repository URLs
helm_repositories:
  prometheus: "https://prometheus-community.github.io/helm-charts"
  grafana: "https://grafana.github.io/helm-charts"

# Ingress Konfiguration
ingress_class: "nginx"
monitoring_ingress_paths:
  prometheus: "/prometheus"
  grafana: "/grafana"
  alertmanager: "/alertmanager"

# k3s Konfiguration
k3s_write_kubeconfig_mode: "644"
k3s_token_file: "/tmp/node-token"
k3s_server_port: "6443"

# Timeouts und Retries (ERHÖHT für stabileres Deployment)
helm_install_timeout: "900s"
k3s_ready_retries: 5
k3s_ready_delay: 45
helm_install_retries: 3
helm_install_delay: 90

# Nginx Ingress Controller Version
nginx_ingress_version: "controller-v1.8.1"

# Ergänzung zu defaults/main.yaml - Hadoop Services Konfiguration mit NodePorts

# Hadoop Cluster Konfiguration (erweitert)
hadoop:
  namespace: "hadoop-cluster"
  chart_path: "/tmp/helm/data_lake"
  release_name: "hadoop-data-lake"
  
  # HDFS Konfiguration
  hdfs:
    replication_factor: 2
    block_size: "128m"
    namenode_heap_size: "2g"
    datanode_heap_size: "2g"
    
  # YARN Konfiguration
  yarn:
    resource_manager_heap_size: "2g"
    node_manager_heap_size: "2g"
    
  # Storage Konfiguration
  storage:
    namenode_size: "20Gi"
    datanode_size: "100Gi"
    storage_class: "local-path"
    
  # Services - NodePort Konfiguration für externen Zugriff
  services:
    namenode_ui_port: 30091      # Externer Port für NameNode Web UI
    resource_manager_ui_port: 30092  # Externer Port für ResourceManager Web UI
    spark_history_ui_port: 30093     # Externer Port für Spark History Server
    
    # Interne Service-Ports (für Cluster-Kommunikation)
    namenode_rpc_port: 9000
    namenode_http_port: 9870
    resourcemanager_web_port: 8088
    spark_history_port: 18080

# Spark Konfiguration für Big Data Processing (erweitert)
spark:
  namespace: "{{ hadoop.namespace }}"
  executor_instances: 2
  executor_cores: 2
  executor_memory: "2g"
  driver_memory: "1g"
  
  # Spark History Server Konfiguration
  history_server:
    enabled: true
    log_directory: "hdfs://hadoop-namenode:9000/spark-logs"
    external_access: true
    nodeport: 30093
  
# Data Ingestion Konfiguration (VEREINFACHT)
data_ingestion:
  datasets:
    - name: "project_tasks"
      source: "/tmp/sample-data/avro_dataset1_project_tasks.json"
      hdfs_path: "/datalake/datasets/project_tasks/"
    - name: "research_activities"
      source: "/tmp/sample-data/avro_dataset2_research_activities.json"
      hdfs_path: "/datalake/datasets/research_activities/"
    - name: "schema"
      source: "/tmp/sample-data/avro_schema_tasks.json"
      hdfs_path: "/datalake/schemas/"
  
  # HDFS Verzeichnisstruktur
  hdfs_directories:
    - "/datalake"
    - "/datalake/datasets"
    - "/datalake/datasets/project_tasks"
    - "/datalake/datasets/research_activities"
    - "/datalake/schemas"
    - "/datalake/raw"
    - "/datalake/processed"
    - "/datalake/archive"
    - "/datalake/metadata"

# Container Images für Big Data
bigdata_images:
  hadoop: "apache/hadoop:3"
  spark: "bitnami/spark:3.4"

# Monitoring für Hadoop (vereinfacht)
hadoop_monitoring:
  enabled: false  # Erstmal deaktiviert für einfacheres Deployment
  scrape_interval: "30s"